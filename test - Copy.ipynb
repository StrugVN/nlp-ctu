{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa75fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "import py_vncorenlp\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from math import log\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import re\n",
    "import unicodedata\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[\"nlp\"]\n",
    "\n",
    "article_collection = db[\"article\"]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbf6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_collection = db[\"article_tf_idf\"]\n",
    "list_tf_idf = list(tf_idf_collection.find({}))\n",
    "\n",
    "\n",
    "rows = []\n",
    "for doc in list_tf_idf:\n",
    "    article_id = doc['articleId']\n",
    "    tf_idf = doc.get('tf_idf', {})\n",
    "    tf_idf['articleId'] = article_id\n",
    "    rows.append(tf_idf)\n",
    "\n",
    "df_tf_idf_full = pd.DataFrame(rows)\n",
    "\n",
    "df_tf_idf_full.set_index('articleId', inplace=True)\n",
    "df_tf_idf_full = df_tf_idf_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab066aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_collection = db[\"article\"]\n",
    "list_articles = list(article_collection.find({}))\n",
    "\n",
    "df_articles = pd.DataFrame(list_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52d5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(token, model, topn=5):\n",
    "    expanded_tokens = {token}  \n",
    "\n",
    "    if token in model.wv:\n",
    "        similar_words = model.wv.most_similar(token, topn=topn)\n",
    "        for word, _ in similar_words:\n",
    "            expanded_tokens.add(word.replace('_', ' ')) \n",
    "\n",
    "    return expanded_tokens\n",
    "\n",
    "\n",
    "def expand_query_enhanced(token, model, topn=5, similarity_threshold=0.5):\n",
    "    expanded_tokens = [(token, 1.0)]\n",
    "    \n",
    "    if token in model.wv:\n",
    "        similar_words = model.wv.most_similar(token, topn=topn)\n",
    "        for word, similarity in similar_words:\n",
    "            if similarity > similarity_threshold:\n",
    "                clean_word = word.replace('_', ' ')\n",
    "                expanded_tokens.append((clean_word, similarity))\n",
    "    \n",
    "    return expanded_tokens\n",
    "\n",
    "def should_expand_token(token, stopwords, min_length=3):\n",
    "    if token.lower() in stopwords:\n",
    "        return False\n",
    "    if len(token) < min_length:\n",
    "        return False\n",
    "    if token.isnumeric():\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d2039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_by_query(query, tf_idf, word_model, tokenizer, stopwords, expansion_weight=0.25):\n",
    "    # Tokenize query\n",
    "    segmented = tokenizer.word_segment(query)\n",
    "    query_tokens = []\n",
    "    for sentence in segmented:\n",
    "        words = sentence.split()\n",
    "        words = [w.replace(\"_\", \" \") for w in words]\n",
    "        words = [w.lower() for w in words if w.lower() not in stopwords]\n",
    "        query_tokens.extend(words)\n",
    "\n",
    "    word_counts = {}\n",
    "\n",
    "    for token in query_tokens:\n",
    "        word_counts[token] = word_counts.get(token, 0) + 1 \n",
    "        expanded_tokens = expand_query(token, word_model, topn=5)\n",
    "        for expanded in expanded_tokens:\n",
    "            if expanded != token and expanded not in stopwords:\n",
    "                word_counts[expanded] = word_counts.get(expanded, 0) + expansion_weight \n",
    "\n",
    "    total_terms = sum(word_counts.values())\n",
    "    if total_terms == 0:\n",
    "        return []\n",
    "\n",
    "    word_list = tf_idf.columns\n",
    "    query_vector = np.zeros(len(word_list))\n",
    "\n",
    "    for i, term in enumerate(word_list):\n",
    "        if term in word_counts:\n",
    "            query_vector[i] = word_counts[term] / total_terms\n",
    "\n",
    "    cosin_sim = cosine_similarity([query_vector], tf_idf.values)[0]\n",
    "\n",
    "    article_ids = tf_idf.index.tolist()\n",
    "    ranked = sorted(zip(article_ids, cosin_sim), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102a4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "import os\n",
    "original_cwd = os.getcwd()\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\"], save_dir=os.path.join(original_cwd, \"vncorenlp\"))\n",
    "os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e5a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip().lower() for line in f if line.strip())\n",
    "stopwords.add('sto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3381d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_by_query_enhanced(query, tf_idf, word_model, tokenizer, stopwords, \n",
    "                                   base_expansion_weight=0.3, \n",
    "                                   adaptive_expansion=True,\n",
    "                                   similarity_threshold=0.5):\n",
    "    \n",
    "    segmented = tokenizer.word_segment(query)\n",
    "    query_tokens = []\n",
    "    for sentence in segmented:\n",
    "        words = sentence.split()\n",
    "        words = [w.replace(\"_\", \" \") for w in words]\n",
    "        words = [w.lower() for w in words if w.lower() not in stopwords]\n",
    "        query_tokens.extend(words)\n",
    "    \n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    if adaptive_expansion:\n",
    "        if len(query_tokens) <= 2:\n",
    "            expansion_weight = base_expansion_weight * 1.5  \n",
    "        elif len(query_tokens) >= 6:\n",
    "            expansion_weight = base_expansion_weight * 0.5  \n",
    "        else:\n",
    "            expansion_weight = base_expansion_weight\n",
    "    else:\n",
    "        expansion_weight = base_expansion_weight\n",
    "    \n",
    "    word_counts = {}\n",
    "    expansion_stats = {'original_terms': 0, 'expanded_terms': 0}\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        # Add original token\n",
    "        word_counts[token] = word_counts.get(token, 0) + 1.0\n",
    "        expansion_stats['original_terms'] += 1\n",
    "        \n",
    "        # Expand token if appropriate\n",
    "        if should_expand_token(token, stopwords):\n",
    "            expanded_tokens = expand_query_enhanced(\n",
    "                token, word_model, \n",
    "                topn=5, \n",
    "                similarity_threshold=similarity_threshold\n",
    "            )\n",
    "            \n",
    "            for expanded_token, similarity in expanded_tokens[1:]: \n",
    "                if expanded_token not in stopwords and expanded_token != token:\n",
    "                    weight = expansion_weight * similarity\n",
    "                    word_counts[expanded_token] = word_counts.get(expanded_token, 0) + weight\n",
    "                    expansion_stats['expanded_terms'] += 1\n",
    "    \n",
    "    total_weight = sum(word_counts.values())\n",
    "    if total_weight == 0:\n",
    "        return []\n",
    "    \n",
    "    word_list = tf_idf.columns\n",
    "    query_vector = np.zeros(len(word_list))\n",
    "    \n",
    "    for i, term in enumerate(word_list):\n",
    "        if term in word_counts:\n",
    "            query_vector[i] = word_counts[term] / total_weight\n",
    "    \n",
    "    cosine_sim = cosine_similarity([query_vector], tf_idf.values)[0]\n",
    "    \n",
    "    article_ids = tf_idf.index.tolist()\n",
    "    ranked = sorted(zip(article_ids, cosine_sim), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked, expansion_stats\n",
    "\n",
    "def search_articles_enhanced(query, top_k=10):\n",
    "    \"\"\"\n",
    "    Enhanced article search with improved query expansion\n",
    "    \"\"\"\n",
    "    results, stats = rank_documents_by_query_enhanced(\n",
    "        query, df_tf_idf_full, word2vec_model, rdrsegmenter, stopwords\n",
    "    )\n",
    "    \n",
    "    # Print expansion statistics for debugging\n",
    "    print(f\"Query expansion stats: {stats}\")\n",
    "    \n",
    "    result_ids = results[:top_k]\n",
    "    result_articles = list(article_collection.find({\n",
    "        \"id\": {\"$in\": [item[0] for item in result_ids]}\n",
    "    }))\n",
    "    \n",
    "    return result_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5513d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec_vi_bao_st.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97ace4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_articles(query):\n",
    "    results = rank_documents_by_query(query, df_tf_idf_full, word2vec_model, rdrsegmenter, stopwords)\n",
    "    result_ids = results[:10]\n",
    "    result_articles = list(article_collection.find({\"id\": {\"$in\": [item[0] for item in result_ids]}}))\n",
    "    return result_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc712e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query expansion stats: {'original_terms': 2, 'expanded_terms': 10}\n",
      "Tích cực thực hiện chương trình phòng, chống tai nạn, thương tích trẻ em\n",
      "--------------------------------------------------------------------------------\n",
      "Tăng cường kiểm soát tốc độ phương tiện tuyến Quốc lộ Quản lộ Phụng Hiệp\n",
      "--------------------------------------------------------------------------------\n",
      "Tạo môi trường an toàn cho trẻ em vùng sông nước TX. Ngã Năm\n",
      "--------------------------------------------------------------------------------\n",
      "Xây dựng ngôi nhà an toàn cho trẻ\n",
      "--------------------------------------------------------------------------------\n",
      "Huyện Châu Thành phấn đấu giảm tai nạn giao thông cả 3 tiêu chí\n",
      "--------------------------------------------------------------------------------\n",
      "Xe cộ lúa cán cháu bé 6 tuổi tử vong tại chỗ\n",
      "--------------------------------------------------------------------------------\n",
      "Nhiều hoạt động trong phòng, chống tai nạn thương tích trẻ em\n",
      "--------------------------------------------------------------------------------\n",
      "Đáng lo ngại về tai nạn giao thông trên tuyến Quốc lộ 1A\n",
      "--------------------------------------------------------------------------------\n",
      "Thăm nạn nhân bị tai nạn giao thông có hoàn cảnh khó khăn\n",
      "--------------------------------------------------------------------------------\n",
      "Tai nạn giao thông trên địa bàn huyện Thạnh Trị giảm sâu\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for article in search_articles_enhanced('tai nạn trên quốc lộ'):\n",
    "    print(article['title'])\n",
    "    # print('https://baosoctrang.org.vn' + article['pageUrl'])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac73720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ContextAwareQueryExpander:\n",
    "    def __init__(self, word_model, tf_idf_matrix, min_phrase_freq=5):\n",
    "        self.word_model = word_model\n",
    "        self.tf_idf_matrix = tf_idf_matrix\n",
    "        self.min_phrase_freq = min_phrase_freq\n",
    "        self._build_phrase_vocabulary()\n",
    "        self._build_context_relationships()\n",
    "    \n",
    "    def _build_phrase_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Extract common phrases from TF-IDF vocabulary\n",
    "        \"\"\"\n",
    "        self.phrase_vocab = {}\n",
    "        vocabulary = self.tf_idf_matrix.columns\n",
    "        \n",
    "        # Find multi-word terms that appear frequently enough\n",
    "        for term in vocabulary:\n",
    "            if ' ' in term and len(term.split()) <= 4:  # 2-4 word phrases\n",
    "                # Check if this phrase appears frequently\n",
    "                phrase_scores = self.tf_idf_matrix[term]\n",
    "                non_zero_count = (phrase_scores > 0).sum()\n",
    "                \n",
    "                if non_zero_count >= self.min_phrase_freq:\n",
    "                    words = term.split()\n",
    "                    self.phrase_vocab[term] = {\n",
    "                        'words': words,\n",
    "                        'frequency': non_zero_count,\n",
    "                        'avg_score': phrase_scores.mean()\n",
    "                    }\n",
    "    \n",
    "    def _build_context_relationships(self):\n",
    "        \"\"\"\n",
    "        Build relationships between words based on co-occurrence in phrases\n",
    "        \"\"\"\n",
    "        self.context_relationships = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        for phrase, info in self.phrase_vocab.items():\n",
    "            words = info['words']\n",
    "            phrase_strength = info['avg_score'] * info['frequency']\n",
    "            \n",
    "            # Create bidirectional relationships between words in phrases\n",
    "            for i, word1 in enumerate(words):\n",
    "                for j, word2 in enumerate(words):\n",
    "                    if i != j:\n",
    "                        # Distance penalty: closer words have stronger relationship\n",
    "                        distance_penalty = 1.0 / (abs(i - j) + 1)\n",
    "                        strength = phrase_strength * distance_penalty\n",
    "                        self.context_relationships[word1][word2] += strength\n",
    "    \n",
    "    def detect_query_phrases(self, query_tokens):\n",
    "        \"\"\"\n",
    "        Dynamically detect phrases in the query based on vocabulary\n",
    "        \"\"\"\n",
    "        detected_phrases = []\n",
    "        query_text = ' '.join(query_tokens)\n",
    "        \n",
    "        # Sort phrases by length (longer first) to avoid partial matches\n",
    "        sorted_phrases = sorted(self.phrase_vocab.keys(), \n",
    "                              key=lambda x: len(x.split()), reverse=True)\n",
    "        \n",
    "        used_positions = set()\n",
    "        \n",
    "        for phrase in sorted_phrases:\n",
    "            # Find all occurrences of this phrase in query\n",
    "            pattern = r'\\b' + re.escape(phrase) + r'\\b'\n",
    "            matches = list(re.finditer(pattern, query_text, re.IGNORECASE))\n",
    "            \n",
    "            for match in matches:\n",
    "                start_pos = len(query_text[:match.start()].split())\n",
    "                end_pos = start_pos + len(phrase.split())\n",
    "                \n",
    "                # Check if this position is already used by a longer phrase\n",
    "                pos_range = set(range(start_pos, end_pos))\n",
    "                if not pos_range.intersection(used_positions):\n",
    "                    detected_phrases.append({\n",
    "                        'phrase': phrase,\n",
    "                        'positions': (start_pos, end_pos),\n",
    "                        'importance': self.phrase_vocab[phrase]['avg_score'],\n",
    "                        'frequency': self.phrase_vocab[phrase]['frequency']\n",
    "                    })\n",
    "                    used_positions.update(pos_range)\n",
    "        \n",
    "        # Also try to construct phrases from consecutive query tokens\n",
    "        self._add_constructed_phrases(query_tokens, detected_phrases, used_positions)\n",
    "        \n",
    "        return detected_phrases\n",
    "    \n",
    "    def _add_constructed_phrases(self, query_tokens, detected_phrases, used_positions):\n",
    "        \"\"\"\n",
    "        Try to construct meaningful phrases from consecutive tokens even if not in vocab\n",
    "        \"\"\"\n",
    "        # Try all possible consecutive combinations (longest first)\n",
    "        for length in range(min(len(query_tokens), 4), 2, -1):  # Try 4,3 word phrases first\n",
    "            for i in range(len(query_tokens) - length + 1):\n",
    "                candidate_phrase = ' '.join(query_tokens[i:i+length])\n",
    "                pos_range = set(range(i, i+length))\n",
    "                \n",
    "                # Skip if positions already used by a detected phrase\n",
    "                if pos_range.intersection(used_positions):\n",
    "                    continue\n",
    "                \n",
    "                # Check if this constructed phrase might be meaningful\n",
    "                if self._is_meaningful_phrase(candidate_phrase, query_tokens):\n",
    "                    detected_phrases.append({\n",
    "                        'phrase': candidate_phrase,\n",
    "                        'positions': (i, i+length),\n",
    "                        'importance': 3.0,  # Higher importance for constructed phrases\n",
    "                        'frequency': 15,  # Assume reasonable frequency\n",
    "                        'constructed': True\n",
    "                    })\n",
    "                    used_positions.update(pos_range)\n",
    "                    print(f\"Constructed phrase: '{candidate_phrase}'\")  # Debug\n",
    "                    # Continue to try other non-overlapping phrases\n",
    "    \n",
    "    def _is_meaningful_phrase(self, phrase, query_tokens=None):\n",
    "        \"\"\"\n",
    "        Heuristics to determine if a constructed phrase might be meaningful\n",
    "        \"\"\"\n",
    "        words = phrase.split()\n",
    "        \n",
    "        # For short queries, any multi-word combination could be meaningful\n",
    "        if query_tokens and len(query_tokens) <= 4:\n",
    "            return True\n",
    "            \n",
    "        # At least one word should be in our word2vec model\n",
    "        model_coverage = sum(1 for word in words if word in self.word_model.wv)\n",
    "        if model_coverage == 0:\n",
    "            return False\n",
    "            \n",
    "        # For 3+ word phrases, be more permissive\n",
    "        if len(words) >= 3:\n",
    "            return True\n",
    "            \n",
    "        return model_coverage >= 1\n",
    "    \n",
    "    def get_contextual_expansions(self, word, context_words, topn=5):\n",
    "        \"\"\"\n",
    "        Get expansions for a word considering its context\n",
    "        \"\"\"\n",
    "        expansions = []\n",
    "        \n",
    "        # Get Word2Vec expansions\n",
    "        if word in self.word_model.wv:\n",
    "            similar_words = self.word_model.wv.most_similar(word, topn=topn*2)\n",
    "            \n",
    "            for candidate, similarity in similar_words:\n",
    "                candidate = candidate.replace('_', ' ')\n",
    "                \n",
    "                # Calculate context relevance\n",
    "                context_score = 0.0\n",
    "                for context_word in context_words:\n",
    "                    if context_word in self.context_relationships[candidate]:\n",
    "                        context_score += self.context_relationships[candidate][context_word]\n",
    "                    if candidate in self.context_relationships[context_word]:\n",
    "                        context_score += self.context_relationships[context_word][candidate]\n",
    "                \n",
    "                # Normalize context score\n",
    "                context_score = context_score / (len(context_words) + 1)\n",
    "                \n",
    "                # Combined score: word2vec similarity + context relevance\n",
    "                combined_score = similarity * 0.7 + min(context_score * 0.3, 0.3)\n",
    "                \n",
    "                expansions.append((candidate, combined_score))\n",
    "        \n",
    "        # Sort by combined score and return top candidates\n",
    "        expansions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return expansions[:topn]\n",
    "    \n",
    "    def expand_query(self, query_tokens, stopwords, base_weight=1.0, \n",
    "                    phrase_weight=3.0, expansion_weight=0.3):\n",
    "        \"\"\"\n",
    "        Expand query with context awareness - prioritize longer, more specific phrases\n",
    "        \"\"\"\n",
    "        word_weights = defaultdict(float)\n",
    "        \n",
    "        # Detect phrases first\n",
    "        detected_phrases = self.detect_query_phrases(query_tokens)\n",
    "        \n",
    "        # Sort phrases by length (longer = more specific = higher priority)\n",
    "        detected_phrases.sort(key=lambda x: len(x['phrase'].split()), reverse=True)\n",
    "        \n",
    "        # Track which tokens are part of phrases\n",
    "        phrase_tokens = set()\n",
    "        phrase_coverage = {}  # Track which tokens are covered by which phrases\n",
    "        \n",
    "        for phrase_info in detected_phrases:\n",
    "            phrase = phrase_info['phrase']\n",
    "            importance = phrase_info.get('importance', 2.0)\n",
    "            frequency = phrase_info.get('frequency', 10)\n",
    "            is_constructed = phrase_info.get('constructed', False)\n",
    "            \n",
    "            # Higher weight for longer, more specific phrases\n",
    "            phrase_length_bonus = len(phrase.split()) * 0.5\n",
    "            phrase_final_weight = phrase_weight * (1 + np.log(frequency)) + phrase_length_bonus\n",
    "            \n",
    "            # Even higher bonus for constructed phrases (like \"giáo dục mầm non\")\n",
    "            if is_constructed:\n",
    "                phrase_final_weight *= 1.5\n",
    "            \n",
    "            word_weights[phrase] += phrase_final_weight\n",
    "            \n",
    "            # Track phrase coverage\n",
    "            phrase_words = phrase.split()\n",
    "            for i, word in enumerate(phrase_words):\n",
    "                phrase_tokens.add(word)\n",
    "                if word not in phrase_coverage:\n",
    "                    phrase_coverage[word] = []\n",
    "                phrase_coverage[word].append({\n",
    "                    'phrase': phrase, \n",
    "                    'length': len(phrase_words),\n",
    "                    'position': i,\n",
    "                    'weight': phrase_final_weight\n",
    "                })\n",
    "        \n",
    "        # Process individual tokens with phrase context awareness\n",
    "        for i, token in enumerate(query_tokens):\n",
    "            if token.lower() in stopwords:\n",
    "                continue\n",
    "            \n",
    "            # Determine token weight based on phrase membership\n",
    "            token_weight = base_weight\n",
    "            \n",
    "            if token in phrase_coverage:\n",
    "                # Token is part of one or more phrases\n",
    "                # Reduce individual token weight if it's part of a longer phrase\n",
    "                max_phrase_length = max(pc['length'] for pc in phrase_coverage[token])\n",
    "                if max_phrase_length >= 3:\n",
    "                    # Significantly reduce weight for tokens in 3+ word phrases\n",
    "                    token_weight *= 0.3\n",
    "                elif max_phrase_length == 2:\n",
    "                    # Moderately reduce for 2-word phrases\n",
    "                    token_weight *= 0.6\n",
    "                    \n",
    "                # But still give some boost for being in a phrase\n",
    "                token_weight *= 1.2\n",
    "            \n",
    "            word_weights[token] += token_weight\n",
    "            \n",
    "            # Get context for expansion\n",
    "            context_words = []\n",
    "            \n",
    "            # Add surrounding words as context\n",
    "            for j in range(max(0, i-2), min(len(query_tokens), i+3)):\n",
    "                if j != i and query_tokens[j].lower() not in stopwords:\n",
    "                    context_words.append(query_tokens[j])\n",
    "            \n",
    "            # Add words from detected phrases as context (prioritize longer phrases)\n",
    "            for phrase_info in detected_phrases:\n",
    "                phrase_words = phrase_info['phrase'].split()\n",
    "                if token in phrase_words:\n",
    "                    context_words.extend([w for w in phrase_words if w != token])\n",
    "            \n",
    "            # Get contextual expansions (reduce expansion for tokens in long phrases)\n",
    "            if context_words:\n",
    "                expansion_factor = expansion_weight\n",
    "                if token in phrase_coverage:\n",
    "                    max_phrase_length = max(pc['length'] for pc in phrase_coverage[token])\n",
    "                    if max_phrase_length >= 3:\n",
    "                        expansion_factor *= 0.5  # Less expansion for specific phrase tokens\n",
    "                \n",
    "                expansions = self.get_contextual_expansions(token, context_words)\n",
    "                \n",
    "                for expanded_word, score in expansions:\n",
    "                    if expanded_word not in stopwords and expanded_word != token:\n",
    "                        expansion_final_weight = expansion_factor * score\n",
    "                        word_weights[expanded_word] += expansion_final_weight\n",
    "        \n",
    "        return dict(word_weights)\n",
    "\n",
    "def rank_documents_context_aware(query, tf_idf, word_model, tokenizer, stopwords, \n",
    "                                expander=None):\n",
    "    \"\"\"\n",
    "    Rank documents using context-aware query expansion\n",
    "    \"\"\"\n",
    "    # Initialize expander if not provided\n",
    "    if expander is None:\n",
    "        expander = ContextAwareQueryExpander(word_model, tf_idf)\n",
    "    \n",
    "    # Tokenize query\n",
    "    segmented = tokenizer.word_segment(query)\n",
    "    query_tokens = []\n",
    "    for sentence in segmented:\n",
    "        words = sentence.split()\n",
    "        words = [w.replace(\"_\", \" \") for w in words]\n",
    "        words = [w.lower() for w in words]\n",
    "        query_tokens.extend(words)\n",
    "    \n",
    "    if not query_tokens:\n",
    "        return [], {}\n",
    "    \n",
    "    # Expand query with context awareness\n",
    "    word_weights = expander.expand_query(query_tokens, stopwords)\n",
    "    \n",
    "    # Create query vector\n",
    "    word_list = tf_idf.columns\n",
    "    query_vector = np.zeros(len(word_list))\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = sum(word_weights.values())\n",
    "    if total_weight == 0:\n",
    "        return [], {}\n",
    "    \n",
    "    # Fill query vector\n",
    "    matched_terms = []\n",
    "    for i, term in enumerate(word_list):\n",
    "        if term in word_weights:\n",
    "            query_vector[i] = word_weights[term] / total_weight\n",
    "            matched_terms.append((term, word_weights[term]))\n",
    "    \n",
    "    # Calculate similarity\n",
    "    cosine_sim = cosine_similarity([query_vector], tf_idf.values)[0]\n",
    "    \n",
    "    # Rank documents\n",
    "    article_ids = tf_idf.index.tolist()\n",
    "    ranked = sorted(zip(article_ids, cosine_sim), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Statistics for debugging\n",
    "    stats = {\n",
    "        'original_tokens': query_tokens,\n",
    "        'detected_phrases': expander.detect_query_phrases(query_tokens) if hasattr(expander, 'detect_query_phrases') else [],\n",
    "        'matched_terms': sorted(matched_terms, key=lambda x: x[1], reverse=True)[:10],\n",
    "        'total_terms': len(word_weights)\n",
    "    }\n",
    "    \n",
    "    return ranked, stats\n",
    "\n",
    "# Global expander instance (initialize once for efficiency)\n",
    "_global_expander = None\n",
    "\n",
    "def get_expander(tf_idf, word_model):\n",
    "    global _global_expander\n",
    "    if _global_expander is None:\n",
    "        print(\"Building context-aware expander (one-time setup)...\")\n",
    "        _global_expander = ContextAwareQueryExpander(word_model, tf_idf)\n",
    "        print(f\"Found {len(_global_expander.phrase_vocab)} common phrases\")\n",
    "    return _global_expander\n",
    "\n",
    "def search_articles_context_aware(query, top_k=10, debug=True):\n",
    "    \"\"\"\n",
    "    Context-aware article search\n",
    "    \"\"\"\n",
    "    expander = get_expander(df_tf_idf_full, word2vec_model)\n",
    "    \n",
    "    results, stats = rank_documents_context_aware(\n",
    "        query, df_tf_idf_full, word2vec_model, rdrsegmenter, stopwords, expander\n",
    "    )\n",
    "    \n",
    "    # Debug output\n",
    "    if debug:\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Original tokens: {stats['original_tokens']}\")\n",
    "        print(f\"Detected phrases: {[p['phrase'] for p in stats['detected_phrases']]}\")\n",
    "        print(f\"Phrase details: {stats['detected_phrases']}\")\n",
    "        print(f\"Top matched terms: {stats['matched_terms'][:5]}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    result_ids = results[:top_k]\n",
    "    result_articles = list(article_collection.find({\n",
    "        \"id\": {\"$in\": [item[0] for item in result_ids]}\n",
    "    }))\n",
    "    \n",
    "    return result_articles\n",
    "\n",
    "# Example usage:\n",
    "# results = search_articles_context_aware(\"tai nạn giao thông\")\n",
    "# results = search_articles_context_aware(\"bệnh viện nhi đồng\") \n",
    "# results = search_articles_context_aware(\"giáo dục mầm non\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0d8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# results = search_articles_context_aware(\"tai nạn giao thông\")\n",
    "# results = search_articles_context_aware(\"bệnh viện nhi đồng\") \n",
    "# results = search_articles_context_aware(\"giáo dục mầm non\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "127c56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building context-aware expander (one-time setup)...\n",
      "Found 5837 common phrases\n",
      "\n",
      "Query: 'giáo dục mầm non'\n",
      "Original tokens: ['giáo dục', 'mầm non']\n",
      "Detected phrases: ['giáo dục', 'mầm non']\n",
      "Phrase details: [{'phrase': 'giáo dục', 'positions': (0, 2), 'importance': 0.0017599859007624604, 'frequency': 1195}, {'phrase': 'mầm non', 'positions': (2, 4), 'importance': 0.0003358951851875883, 'frequency': 81}]\n",
      "Top matched terms: [('giáo dục', 26.25770439309683), ('mầm non', 18.183347464017316), ('tiểu học', 0.16712696850299835), ('mẫu giáo', 0.1639937442541122), ('công lập', 0.15711144804954527)]\n",
      "--------------------------------------------------\n",
      "Thành phố Sóc Trăng họp mặt kỷ niệm 42 năm ngày Nhà giáo Việt Nam\n",
      "https://baosoctrang.org.vn/giao-duc-dao-tao/202411/thanh-pho-soc-trang-hop-mat-ky-niem-42-nam-ngay-nha-giao-viet-nam-9913c13/\n",
      "--------------------------------------------------------------------------------\n",
      "Khảo sát tình hình xây dựng và phát triển trường mầm non trên địa bàn TP. Sóc Trăng\n",
      "https://baosoctrang.org.vn/thanh-pho-soc-trang-tren-duong-phat-trien/202008/khao-sat-tinh-hinh-xay-dung-va-phat-trien-truong-mam-non-tren-ia-ban-tp-soc-trang-5e10c91/\n",
      "--------------------------------------------------------------------------------\n",
      "“Xây dựng trường mầm non lấy trẻ làm trung tâm” - Giúp trẻ phát triển toàn diện\n",
      "https://baosoctrang.org.vn/thanh-pho-soc-trang-tren-duong-phat-trien/202008/Ixay-dung-truong-mam-non-lay-tre-lam-trung-tamU-giup-tre-phat-trien-toan-dien-9f80ca9/\n",
      "--------------------------------------------------------------------------------\n",
      "Đẩy mạnh xã hội hóa giáo dục mầm non ngoài công lập\n",
      "https://baosoctrang.org.vn/thanh-pho-soc-trang-tren-duong-phat-trien/201712/ay-manh-xa-hoi-hoa-giao-duc-mam-non-ngoai-cong-lap-85d1192/\n",
      "--------------------------------------------------------------------------------\n",
      "Cần đẩy mạnh công tác phân luồng học sinh sau trung học cơ sở\n",
      "https://baosoctrang.org.vn/thi-xa-vinh-chau/201908/can-ay-manh-cong-tac-phan-luong-hoc-sinh-sau-trung-hoc-co-so-ce02512/\n",
      "--------------------------------------------------------------------------------\n",
      "Khảo sát tình hình xây dựng và phát triển trường mầm non trên địa bàn huyện Châu Thành\n",
      "https://baosoctrang.org.vn/huyen-chau-thanh/202008/khao-sat-tinh-hinh-xay-dung-va-phat-trien-truong-mam-non-tren-ia-ban-huyen-chau-thanh-ea3358f/\n",
      "--------------------------------------------------------------------------------\n",
      "Cần quan tâm sắp xếp mạng lưới trường lớp, phấn đấu có thêm trường đạt chuẩn quốc gia\n",
      "https://baosoctrang.org.vn/huyen-ke-sach/201908/can-quan-tam-sap-xep-mang-luoi-truong-lop-phan-au-co-them-truong-at-chuan-quoc-gia-cd63064/\n",
      "--------------------------------------------------------------------------------\n",
      "Đưa công tác phổ biến, giáo dục pháp luật đi vào cuộc sống\n",
      "https://baosoctrang.org.vn/huyen-long-phu/202212/ua-cong-tac-pho-bien-giao-duc-phap-luat-i-vao-cuoc-song-63e39d6/\n",
      "--------------------------------------------------------------------------------\n",
      "Tích cực nâng cao chất lượng chăm sóc, giáo dục trẻ mầm non\n",
      "https://baosoctrang.org.vn/huyen-long-phu/201804/tich-cuc-nang-cao-chat-luong-cham-soc-giao-duc-tre-mam-non-dae3dd2/\n",
      "--------------------------------------------------------------------------------\n",
      "Luôn đổi mới, nâng chất trong công tác phổ biến, giáo dục pháp luật\n",
      "https://baosoctrang.org.vn/huyen-thanh-tri/202301/luon-oi-moi-nang-chat-trong-cong-tac-pho-bien-giao-duc-phap-luat-515195d/\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = search_articles_context_aware(\"giáo dục mầm non\")\n",
    "for article in results:\n",
    "    print(article['title'])\n",
    "    print('https://baosoctrang.org.vn' + article['pageUrl'])\n",
    "    print('-' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
