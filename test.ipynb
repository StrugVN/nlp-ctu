{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa75fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient\n",
    "import py_vncorenlp\n",
    "import re\n",
    "from collections import Counter\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from math import log\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import re\n",
    "import unicodedata\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[\"nlp\"]\n",
    "\n",
    "article_collection = db[\"article\"]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbf6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_collection = db[\"article_tf_idf\"]\n",
    "list_tf_idf = list(tf_idf_collection.find({}))\n",
    "\n",
    "\n",
    "rows = []\n",
    "for doc in list_tf_idf:\n",
    "    article_id = doc['articleId']\n",
    "    tf_idf = doc.get('tf_idf', {})\n",
    "    tf_idf['articleId'] = article_id\n",
    "    rows.append(tf_idf)\n",
    "\n",
    "df_tf_idf_full = pd.DataFrame(rows)\n",
    "\n",
    "df_tf_idf_full.set_index('articleId', inplace=True)\n",
    "df_tf_idf_full = df_tf_idf_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab066aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_collection = db[\"article\"]\n",
    "list_articles = list(article_collection.find({}))\n",
    "\n",
    "df_articles = pd.DataFrame(list_articles)\n",
    "\n",
    "def fix_spacing(text):\n",
    "    text = re.sub(r'([.,!?;:])(?=\\S)', r'\\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def is_writer_signature(sentence):\n",
    "    sentence = sentence.strip().rstrip('.')\n",
    "    \n",
    "    if len(sentence.split()) <= 3 and sentence == sentence.title():\n",
    "        return True\n",
    "    \n",
    "    if re.fullmatch(r'[A-Z]\\.?', sentence) or re.fullmatch(r'[A-Z][a-z]+(\\s[A-Z][a-z]+)?', sentence):\n",
    "        return True\n",
    "\n",
    "    if \"Thực hiện\" in sentence or sentence.upper() == sentence:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def remove_writer_name(text):\n",
    "    sentences = [s.strip() for s in text.strip().split('.') if s.strip()]\n",
    "    if not sentences:\n",
    "        return text\n",
    "    last = sentences[-1]\n",
    "    if is_writer_signature(last):\n",
    "        return '. '.join(sentences[:-1]) + '.' if len(sentences) > 1 else ''\n",
    "    return text\n",
    "\n",
    "# clear stuff like \\n \\xa0... from df['content]\n",
    "df_articles['content'] = df_articles['content'].apply(lambda x: re.sub(r'[\\n\\r\\t\\xa0\\u200b\\u202f]+', ' ', str(x)).strip())\n",
    "# fix spacing\n",
    "df_articles['content'] = df_articles['content'].apply(fix_spacing)\n",
    "# remove writer name\n",
    "df_articles['content'] = df_articles['content'].apply(remove_writer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d52d5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(token, model, topn=5):\n",
    "    expanded_tokens = {token}  \n",
    "\n",
    "    if token in model.wv:\n",
    "        similar_words = model.wv.most_similar(token, topn=topn)\n",
    "        for word, _ in similar_words:\n",
    "            expanded_tokens.add(word.replace('_', ' ')) \n",
    "\n",
    "    return expanded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d2039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_by_query(query, tf_idf, word_model, tokenizer, stopwords, expansion_weight=0.25):\n",
    "    # Tokenize query\n",
    "    segmented = tokenizer.word_segment(query)\n",
    "    query_tokens = []\n",
    "    for sentence in segmented:\n",
    "        words = sentence.split()\n",
    "        words = [w.replace(\"_\", \" \") for w in words]\n",
    "        words = [w.lower() for w in words if w.lower() not in stopwords]\n",
    "        query_tokens.extend(words)\n",
    "\n",
    "    word_counts = {}\n",
    "\n",
    "    for token in query_tokens:\n",
    "        word_counts[token] = word_counts.get(token, 0) + 1 \n",
    "        expanded_tokens = expand_query(token, word_model, topn=5)\n",
    "        for expanded in expanded_tokens:\n",
    "            if expanded != token and expanded not in stopwords:\n",
    "                word_counts[expanded] = word_counts.get(expanded, 0) + expansion_weight \n",
    "\n",
    "    total_terms = sum(word_counts.values())\n",
    "    if total_terms == 0:\n",
    "        return []\n",
    "\n",
    "    word_list = tf_idf.columns\n",
    "    query_vector = np.zeros(len(word_list))\n",
    "\n",
    "    for i, term in enumerate(word_list):\n",
    "        if term in word_counts:\n",
    "            query_vector[i] = word_counts[term] / total_terms\n",
    "\n",
    "    cosin_sim = cosine_similarity([query_vector], tf_idf.values)[0]\n",
    "\n",
    "    article_ids = tf_idf.index.tolist()\n",
    "    ranked = sorted(zip(article_ids, cosin_sim), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bee5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_articles(query):\n",
    "    results = rank_documents_by_query(query, df_tf_idf_full, word2vec_model, rdrsegmenter, stopwords)\n",
    "    result_ids = results[:10]\n",
    "    result_articles = list(article_collection.find({\"id\": {\"$in\": [item[0] for item in result_ids]}}))\n",
    "    return result_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102a4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "import os\n",
    "original_cwd = os.getcwd()\n",
    "rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\", \"pos\", \"ner\", \"parse\"], save_dir=os.path.join(original_cwd, \"vncorenlp\"))\n",
    "os.chdir(original_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194eac01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'index': 1,\n",
       "   'wordForm': 'tình_hình',\n",
       "   'posTag': 'N',\n",
       "   'nerLabel': 'O',\n",
       "   'head': 0,\n",
       "   'depLabel': 'root'},\n",
       "  {'index': 2,\n",
       "   'wordForm': 'giao_thông',\n",
       "   'posTag': 'N',\n",
       "   'nerLabel': 'O',\n",
       "   'head': 1,\n",
       "   'depLabel': 'nmod'},\n",
       "  {'index': 3,\n",
       "   'wordForm': 'ở',\n",
       "   'posTag': 'E',\n",
       "   'nerLabel': 'O',\n",
       "   'head': 1,\n",
       "   'depLabel': 'loc'},\n",
       "  {'index': 4,\n",
       "   'wordForm': 'thành_phố',\n",
       "   'posTag': 'N',\n",
       "   'nerLabel': 'O',\n",
       "   'head': 3,\n",
       "   'depLabel': 'pob'},\n",
       "  {'index': 5,\n",
       "   'wordForm': 'sóc_trăng',\n",
       "   'posTag': 'N',\n",
       "   'nerLabel': 'B-LOC',\n",
       "   'head': 4,\n",
       "   'depLabel': 'nmod'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdrsegmenter.annotate_text('tình hình giao thông ở thành phố sóc trăng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e5a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip().lower() for line in f if line.strip())\n",
    "stopwords.add('sto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d218c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec_vi_bao_st.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3381d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expand_query_enhanced(token, model, topn=5, similarity_threshold=0.5):\n",
    "    expanded_tokens = [(token, 1.0)]\n",
    "    \n",
    "    if token in model.wv:\n",
    "        similar_words = model.wv.most_similar(token, topn=topn)\n",
    "        for word, similarity in similar_words:\n",
    "            if similarity > similarity_threshold:\n",
    "                clean_word = word.replace('_', ' ')\n",
    "                expanded_tokens.append((clean_word, similarity))\n",
    "    \n",
    "    return expanded_tokens\n",
    "\n",
    "def should_expand_token(token, stopwords, min_length=3):\n",
    "    if token.lower() in stopwords:\n",
    "        return False\n",
    "    if len(token) < min_length:\n",
    "        return False\n",
    "    if token.isnumeric():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def rank_documents_by_query_enhanced(query, tf_idf, word_model, tokenizer, stopwords, \n",
    "                                   base_expansion_weight=0.3, \n",
    "                                   adaptive_expansion=True,\n",
    "                                   similarity_threshold=0.7):\n",
    "    \n",
    "    segmented = tokenizer.word_segment(query)\n",
    "    query_tokens = []\n",
    "    for sentence in segmented:\n",
    "        words = sentence.split()\n",
    "        words = [w.replace(\"_\", \" \") for w in words]\n",
    "        words = [w.lower() for w in words if w.lower() not in stopwords]\n",
    "        query_tokens.extend(words)\n",
    "    \n",
    "    if not query_tokens:\n",
    "        return []\n",
    "    \n",
    "    if adaptive_expansion:\n",
    "        if len(query_tokens) <= 2:\n",
    "            expansion_weight = base_expansion_weight * 1.5  \n",
    "        elif len(query_tokens) >= 6:\n",
    "            expansion_weight = base_expansion_weight * 0.5  \n",
    "        else:\n",
    "            expansion_weight = base_expansion_weight\n",
    "    else:\n",
    "        expansion_weight = base_expansion_weight\n",
    "    \n",
    "    word_counts = {}\n",
    "    expansion_stats = {'original_terms': 0, 'expanded_terms': 0}\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        word_counts[token] = word_counts.get(token, 0) + 1.0\n",
    "        expansion_stats['original_terms'] += 1\n",
    "        \n",
    "        if should_expand_token(token, stopwords):\n",
    "            expanded_tokens = expand_query_enhanced(\n",
    "                token, word_model, \n",
    "                topn=5, \n",
    "                similarity_threshold=similarity_threshold\n",
    "            )\n",
    "            \n",
    "            for expanded_token, similarity in expanded_tokens[1:]: \n",
    "                if expanded_token not in stopwords and expanded_token != token:\n",
    "                    weight = expansion_weight * similarity\n",
    "                    word_counts[expanded_token] = word_counts.get(expanded_token, 0) + weight\n",
    "                    expansion_stats['expanded_terms'] += 1\n",
    "    \n",
    "    total_weight = sum(word_counts.values())\n",
    "    if total_weight == 0:\n",
    "        return []\n",
    "    \n",
    "    word_list = tf_idf.columns.tolist()\n",
    "    col_index = {col.lower(): idx for idx, col in enumerate(word_list)}\n",
    "\n",
    "    query_vector = np.zeros(len(word_list))\n",
    "    for tok, wt in word_counts.items():\n",
    "        idx = col_index.get(tok) \n",
    "        if idx is not None:\n",
    "            query_vector[idx] = wt / total_weight\n",
    "    \n",
    "    cosine_sim = cosine_similarity([query_vector], tf_idf.values)[0]\n",
    "    \n",
    "    article_ids = tf_idf.index.tolist()\n",
    "    ranked = sorted(zip(article_ids, cosine_sim), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked, expansion_stats, word_counts\n",
    "\n",
    "def search_articles_enhanced(query, top_k=10):\n",
    "    \"\"\"\n",
    "    Enhanced article search with improved query expansion\n",
    "    \"\"\"\n",
    "    results, stats, query_tokens = rank_documents_by_query_enhanced(\n",
    "        query, df_tf_idf_full, word2vec_model, rdrsegmenter, stopwords\n",
    "    )\n",
    "    \n",
    "    # Print expansion statistics for debugging\n",
    "    print(f\"Query expansion stats: {stats}\")\n",
    "    print(f\"Query tokens: {query_tokens}\")\n",
    "    \n",
    "    result_ids = results[:top_k]\n",
    "    result_articles = list(article_collection.find({\n",
    "        \"id\": {\"$in\": [item[0] for item in result_ids]}\n",
    "    }))\n",
    "    \n",
    "    return result_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5cc712e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query expansion stats: {'original_terms': 1, 'expanded_terms': 0}\n",
      "Query tokens: {'vnpt': 1.0}\n",
      "Thành phố Sóc Trăng phát sóng wifi miễn phí tại các công viên\n",
      "https://baosoctrang.org.vn/thanh-pho-soc-trang-tren-duong-phat-trien/202410/thanh-pho-soc-trang-phat-song-wifi-mien-phi-tai-cac-cong-vien-c7d3d14/\n",
      "--------------------------------------------------------------------------------\n",
      "Thành phố Sóc Trăng phát động hưởng ứng Ngày Chuyển đổi số Quốc gia\n",
      "https://baosoctrang.org.vn/thoi-su/202410/thanh-pho-soc-trang-phat-ong-huong-ung-ngay-chuyen-oi-so-quoc-gia-27231f4/\n",
      "--------------------------------------------------------------------------------\n",
      "Sóc Trăng phát động hưởng ứng Ngày Chuyển đổi số quốc gia\n",
      "https://baosoctrang.org.vn/chuyen-doi-so/202410/soc-trang-phat-ong-huong-ung-ngay-chuyen-oi-so-quoc-gia-6a56292/\n",
      "--------------------------------------------------------------------------------\n",
      "Tổng kết công tác văn hóa, thể thao, du lịch và truyền thanh năm 2021\n",
      "https://baosoctrang.org.vn/huyen-chau-thanh/202202/tong-ket-cong-tac-van-hoa-the-thao-du-lich-va-truyen-thanh-nam-2021-b2134c3/\n",
      "--------------------------------------------------------------------------------\n",
      "Tuyên dương 31 tập thể, cá nhân có thành tích tiêu biểu trong học tập và làm theo Bác\n",
      "https://baosoctrang.org.vn/huyen-ke-sach/202305/tuyen-duong-31-tap-the-ca-nhan-co-thanh-tich-tieu-bieu-trong-hoc-tap-va-lam-theo-bac-b802dbe/\n",
      "--------------------------------------------------------------------------------\n",
      "Tập huấn cán bộ đoàn, đoàn viên, thanh niên nội dung liên quan đến chuyển đổi số\n",
      "https://baosoctrang.org.vn/huyen-ke-sach/202304/tap-huan-can-bo-oan-oan-vien-thanh-nien-noi-dung-lien-quan-en-chuyen-oi-so-1702ddf/\n",
      "--------------------------------------------------------------------------------\n",
      "Đưa hàng Việt về nông thôn huyện Mỹ Tú\n",
      "https://baosoctrang.org.vn/huyen-my-tu/202308/ua-hang-viet-ve-nong-thon-huyen-my-tu-9e327fd/\n",
      "--------------------------------------------------------------------------------\n",
      "Đưa hàng Việt về huyện Cù Lao Dung\n",
      "https://baosoctrang.org.vn/huyen-cu-lao-dung/202207/ua-hang-viet-ve-huyen-cu-lao-dung-2451525/\n",
      "--------------------------------------------------------------------------------\n",
      "Bí thư Tỉnh ủy khảo sát công trình và một số mô hình sản xuất tại Cù Lao Dung\n",
      "https://baosoctrang.org.vn/huyen-cu-lao-dung/202008/bi-thu-tinh-uy-khao-sat-cong-trinh-va-mot-so-mo-hinh-san-xuat-tai-cu-lao-dung-8251615/\n",
      "--------------------------------------------------------------------------------\n",
      "Xã Đại Ân 2 ra mắt mô hình Chợ thanh toán không dùng tiền mặt\n",
      "https://baosoctrang.org.vn/huyen-tran-de/202312/xa-ai-an-2-ra-mat-mo-hinh-cho-thanh-toan-khong-dung-tien-mat-4024489/\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for article in search_articles_enhanced('VNPT'):\n",
    "    print(article['title'])\n",
    "    print('https://baosoctrang.org.vn' + article['pageUrl'])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b187279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tai nạn trên quốc lộ', 1.0)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_query_enhanced('tai nạn trên quốc lộ', word2vec_model, topn=5, similarity_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0e0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer_gpt_vi = GPT2Tokenizer.from_pretrained(\"NlpHUST/gpt2-vietnamese\", cache_dir=\"./transformers_cache\")\n",
    "model_gpt_vi = GPT2LMHeadModel.from_pretrained(\"NlpHUST/gpt2-vietnamese\", cache_dir=\"./transformers_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4804418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Generated text 1\n",
      "\n",
      "tai nạn giao thông mới xảy\n",
      "\n",
      "---\n",
      ">> Generated text 2\n",
      "\n",
      "tai nạn giao thông, tai\n",
      "\n",
      "---\n",
      ">> Generated text 3\n",
      "\n",
      "tai nạn giao thông đường sắt\n",
      "\n",
      "---\n",
      ">> Generated text 4\n",
      "\n",
      "tai nạn giao thông nghiêm trọng\n",
      "\n",
      "---\n",
      ">> Generated text 5\n",
      "\n",
      "tai nạn giao thông” năm\n",
      "\n",
      "---\n",
      ">> Generated text 6\n",
      "\n",
      "tai nạn giao thông, tai\n",
      "\n",
      "---\n",
      ">> Generated text 7\n",
      "\n",
      "tai nạn giao thông” năm\n",
      "\n",
      "---\n",
      ">> Generated text 8\n",
      "\n",
      "tai nạn giao thông đường sắt\n",
      "\n",
      "---\n",
      ">> Generated text 9\n",
      "\n",
      "tai nạn giao thông nghiêm trọng\n",
      "\n",
      "---\n",
      ">> Generated text 10\n",
      "\n",
      "tai nạn giao thông” trong\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"tai nạn giao\"\n",
    "input_ids = tokenizer_gpt_vi.encode(text, return_tensors='pt')\n",
    "\n",
    "sample_outputs = model_gpt_vi.generate(input_ids,pad_token_id=tokenizer_gpt_vi.eos_token_id,\n",
    "                                   do_sample=True,\n",
    "                                   max_length=len(text.split(' ')) + 3,\n",
    "                                   min_length=len(text.split(' ')) + 1,\n",
    "                                   top_k=5,\n",
    "                                   num_beams=1,\n",
    "                                   early_stopping=True,\n",
    "                                   no_repeat_ngram_size=3,\n",
    "                                   num_return_sequences=10)\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\">> Generated text {}\\n\\n{}\".format(i+1, tokenizer_gpt_vi.decode(sample_output.tolist())))\n",
    "    print('\\n---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e0e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Generated text 1\n",
      "\n",
      "tai nạn giao thông mới nhất 24h qua, cập nhật tin nóng thời sự trưa ngày 24/\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
